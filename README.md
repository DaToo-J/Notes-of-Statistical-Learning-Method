https://github.com/DaToo-J/Notes-of-Statistical-Learning-Method/blob/master/1.%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA.md

[internal link](#internal)


# 一、 统计学习方法概论
## 1.1 统计学习

1. 特点：以数据为研究对象，是数据驱动的学科

2. 学习：一个系统能够通过执行某个过程改进它的性能

3. 基本假设：同类数据具有一定的统计规律。

4. 数据：

- 由变量或变量组表示；

- 连续变量 或 **离散变量**；

5. 目的：

- 学习什么样的模型，

- 如何学习模型，

- 模型对数据进行的预测与分析，

- 学习效率如何

6. 方法：

- **监督学习**

- 非监督学习

- 半监督学习

- 强化学习

7. 三概念：

- 假设空间：hypothesis space，学习模型的集合
        - 模型：输入空间到输出空间的映射
        - 假设空间：确定学习的范围

- 评价准则：evaluation criterion，从假设空间选一个最优的模型

- 测试数据：test data

8. 三要素：

- 模型：model，模型的假设空间

- 策略：strategy，模型选择的准则

- 算法：algorithm，模型学习的算法

9. 步骤：

- 得到有限的 **训练数据集合**

- 确定所有可能的模型的 **假设空间**，即学习模型的集合

- 确定模型 **选择的准则**，即学习的策略

- 实现求解最优模型的算法，即学习的算法

- 通过学习方法选择最优模型

- 利用学习的最优模型对新数据进行预测或分析

10. 应用

- 分类、标注、回归

- nlp、信息检索、文本数据挖掘


# internal

## 1.2 监督学习

学习一个模型，使模型对任意输入进行预测

1. 基本概念

- 输入/输出空间：

        -  所有输入与输出可能取值的集合；

        -  可以是同一个空间，也可以是不同的；

        -  normally，输出 远小于 输入

- 特征空间：

        - 特征向量的集合

        - 特征向量，用来表示每个具体的输入，输入是一个实例

        - 特征空间，每一维对应于一个特征

2. 记作：

- 一个输入变量：$x = (x^{(1)},x^{(2)},...,x^{(n)})^{T}$

        - $x^{(i)}$: $x$的第i个特征

- 多个输入变量：$x_i = (x^{(1)}_i,x^{(2)}_i,...,x^{(n)}_i)^T$

        - $x_i$：多个输入变量中的第i个

- 训练集：$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$

        - 测试数据、训练数据：由输入输出对组成；又称样本、样本点


| 预测问题 | input | output | 
|---|---|---|
|回归 | 连续变量 | 连续变量
| 分类 | |有限个离散变量
|标注 | 变量序列 | 变量序列

3. 分布函数：$P(X,Y)$, 输入输出的随机变量遵循 **联合概率分布**
4. 监督学习的模型：

| 模型 | 表示 | 具体预测时的表示|
|---|---|---
|概率模型 | 条件概率分布$P(Y|X)$表示 | $P(y|x)$
| 非概率模型 | 决策函数$Y = f(X)$ | $y=f(x)$

5. 问题形式化

**1. 学习过程**：

- $输入变量/训练数据 \frac{通过学习}{训练得到} 学习模型$
- $由条件概率分布\hat P(Y|X)或决策函数Y = \hat f(X)表示, 即为输入与输出之间的映射关系
$

**2. 预测过程**

- $测试样本集，输入x_{N+1}\frac{模型}{给出} 输出 y_{N+1}$
- $模型： y_{N+1}=argmax\hat P(y_{N+1}|x_{N+1}) 或 y_{N+1} = \hat f(x_{N+1})$




## 1.3 统计学习三要素

*方法 = 模型 + 策略  + 算法*

### **1. 模型**

1. 假设空间 $\mathcal F$ 为 **决策函数** 的集合

- 该集合为： $\mathcal F = \{f | Y = f(X)\}$

        - $X,Y$ : 输入输出空间 $\mathcal X、 \mathcal Y$ 上的变量

- $\mathcal F = \{f|Y = f_\theta(X), \theta \in R^n\}$

        - 此时，$\mathcal F$ 是由一个参数向量决定的函数族

        - 参数向量 $\theta$ 取值于n维欧氏空间 $R^n$

2. 假设空间 $\mathcal F$ 为 **条件概率** 的集合

- 该集合为： $\mathcal F = \{P | P(Y|X)\}$

- $\mathcal F = \{P|P_\theta(Y|X), \theta \in R^n\}$




### **2. 策略**

- 统计学习目标：从假设空间中选取最优模型

- 损失函数度量 **一次预测** 的好坏，风险函数度量 **平均意义下** 的预测好坏


#### **2.1 损失函数 和 风险函数**

- **损失函数**

1. 度量预测值$f(X)$ 和 真实值 $Y$ 的不一致，记作 $L(Y,f(X))$

2. 分类

    - 0-1 损失函数：$L(Y,f(X)) = \{\frac{1, Y\neq f(X)}{0, Y = f(X)}$

    - 平方损失函数： $L(Y,f(X)) = (Y - f(X))^2$

    - 绝对损失函数： $L(Y,f(X)) = |Y - f(X)|$

    - 对数损失函数： $L(Y,P(Y|X)) = - logP(Y|X)$

<p></p>

- **风险函数**
又称期望损失, expected loss, $R_{exp}(f) $

1. 模型 $f(X)$ 关于联合分布 $P(X,Y)$ 平均意义下的损失，对损失函数求期望

2. $R_{exp}(f) = E_{P}[L(Y,f(X))] = \int_{\mathcal X \times \mathcal Y} L(y,f(x))P(x,y)\mathbf dx\mathbf dy$

3. **病态问题**：期望风险最小学习模型需要用到联合概率分布，但联合概率分布 $P(X,Y)$又是未知的。

<p></p>

- **经验风险**
又称 经验损失，empirical loss, $R_{emp}(f)$

1. $R_{emp}(f) = \frac{1}{N} \sum_{i=1}^NL(y_{i},f(x_i))$

2. 期望风险 $R_{exp}(f)$：关于 **联合分布的期望损失**，对损失函数求期望

3. 经验风险$R_{emp}(f)$：关于 **训练样本集的平均损失**， 对损失函数求平均

4. 样本容量 $N$ 趋于无穷时，后者趋于前者；

5. 现实是，样本数目有限，用后者估计前者时需要矫正： 经验风险最小化 和 结构风险最小化




#### **2.2 经验风险最小化 和 结构风险最小化**

- **经验风险最小化**
又 ERM， empirical risk minimization

1. 认为 经验风险最小的模型就是最优模型

2. $min_{f\in\mathcal F} \frac{1}{N} \sum_{i=1}^NL(y_{i},f(x_i))$

3. 模型=条件概率分布，损失函数=对数损失函数，经验风险最小化=**极大似然估计**

4. 效果未必好，会出现‘过拟合’现象


<p></p>


- **结构风险最小化**
又SRM，structural risk minimization

1. 为防止‘过拟合’，在经验风险上加上 表示模型复杂度的正则化项或罚项， 则为 结构风险

2. 结构风险 $R_{srm}(f) = R_{emp}(f) + \lambda J(f) = \frac{1}{N} \sum_{i=1}^NL(y_{i},f(x_i))  + \lambda J(f) $

    - $J(f)$：模型的复杂度；模型越复杂，复杂度越大；

            - 表示对 复杂模型的惩罚

    - $\lambda$：权衡经验风险和模型复杂度

            - 结构风险需要经验风险和复杂度同时小

3. 模型=条件概率，损失函数=对数损失函数，模型复杂度=模型先验概率，结构风险最小化= **最大后验概率估计**

4. 认为 结构风险最小的模型就是最优模型
5. $min_{f\in\mathcal F} \frac{1}{N} \sum_{i=1}^NL(y_{i},f(x_i))  + \lambda J(f)$



## 1.4 模型评估于模型选择

统计学习的目的是对已知数据和位置数据都能有很好的预测能力

所以，基于损失函数的训练误差 和 测试误差就成为学习方法的评估标准

### **1. 训练误差 和 测试误差**

- **训练误差**

1. 是模型 $Y = \hat f(X)$ 关于 *训练数据集* 的平均损失

2. $R_{emp}(\hat f) = \frac{1}{N} \sum_{i=1}^NL(y_i,\hat f(x_i))$

3. $N$:是训练样本容量




internal link is here
