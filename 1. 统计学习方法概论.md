
[ 一、 统计学习方法概论2](#一、-统计学习方法概论2)
[1.3 statistical](#1.3-statistical)

# 一、 统计学习方法概论
## 1.1 统计学习

1. 特点：以数据为研究对象，是数据驱动的学科

2. 学习：一个系统能够通过执行某个过程改进它的性能

3. 基本假设：同类数据具有一定的统计规律。

4. 数据：

- 由变量或变量组表示；

- 连续变量 或 **离散变量**；

5. 目的：

- 学习什么样的模型，

- 如何学习模型，

- 模型对数据进行的预测与分析，

- 学习效率如何

6. 方法：

- **监督学习**

- 非监督学习

- 半监督学习

- 强化学习

7. 三概念：

- 假设空间：hypothesis space，学习模型的集合
        - 模型：输入空间到输出空间的映射
        - 假设空间：确定学习的范围

- 评价准则：evaluation criterion，从假设空间选一个最优的模型

- 测试数据：test data

8. 三要素：

- 模型：model，模型的假设空间

- 策略：strategy，模型选择的准则

- 算法：algorithm，模型学习的算法

9. 步骤：

- 得到有限的 **训练数据集合**

- 确定所有可能的模型的 **假设空间**，即学习模型的集合

- 确定模型 **选择的准则**，即学习的策略

- 实现求解最优模型的算法，即学习的算法

- 通过学习方法选择最优模型

- 利用学习的最优模型对新数据进行预测或分析

10. 应用

- 分类、标注、回归

- nlp、信息检索、文本数据挖掘




## 1.2 监督学习

学习一个模型，使模型对任意输入进行预测

1. 基本概念

- 输入/输出空间：

        -  所有输入与输出可能取值的集合；

        -  可以是同一个空间，也可以是不同的；

        -  normally，输出 远小于 输入

- 特征空间：

        - 特征向量的集合

        - 特征向量，用来表示每个具体的输入，输入是一个实例

        - 特征空间，每一维对应于一个特征

2. 记作：

- 一个输入变量：$x = (x^{(1)},x^{(2)},...,x^{(n)})^{T}$

        - $x^{(i)}$: $x$的第i个特征

- 多个输入变量：$x_i = (x^{(1)}_i,x^{(2)}_i,...,x^{(n)}_i)^T$

        - $x_i$：多个输入变量中的第i个

- 训练集：$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$

        - 测试数据、训练数据：由输入输出对组成；又称样本、样本点


| 预测问题 | input | output | 
|---|---|---|
|回归 | 连续变量 | 连续变量
| 分类 | |有限个离散变量
|标注 | 变量序列 | 变量序列

3. 分布函数：$P(X,Y)$, 输入输出的随机变量遵循 **联合概率分布**
4. 监督学习的模型：

| 模型 | 表示 | 具体预测时的表示|
|---|---|---
|概率模型 | 条件概率分布$P(Y|X)$表示 | $P(y|x)$
| 非概率模型 | 决策函数$Y = f(X)$ | $y=f(x)$

5. 问题形式化

**1. 学习过程**：

- $输入变量/训练数据 \frac{通过学习}{训练得到} 学习模型$
- $由条件概率分布\hat P(Y|X)或决策函数Y = \hat f(X)表示, 即为输入与输出之间的映射关系
$

**2. 预测过程**

- $测试样本集，输入x_{N+1}\frac{模型}{给出} 输出 y_{N+1}$
- $模型： y_{N+1}=argmax\hat P(y_{N+1}|x_{N+1}) 或 y_{N+1} = \hat f(x_{N+1})$




## 1.3 statistical

*方法 = 模型 + 策略  + 算法*

### **1. 模型**

1. 假设空间 $\mathcal F$ 为 **决策函数** 的集合

- 该集合为： $\mathcal F = \{f | Y = f(X)\}$

        - $X,Y$ : 输入输出空间 $\mathcal X、 \mathcal Y$ 上的变量

- $\mathcal F = \{f|Y = f_\theta(X), \theta \in R^n\}$

        - 此时，$\mathcal F$ 是由一个参数向量决定的函数族

        - 参数向量 $\theta$ 取值于n维欧氏空间 $R^n$

2. 假设空间 $\mathcal F$ 为 **条件概率** 的集合

- 该集合为： $\mathcal F = \{P | P(Y|X)\}$

- $\mathcal F = \{P|P_\theta(Y|X), \theta \in R^n\}$




### **2. 策略**

- 统计学习目标：从假设空间中选取最优模型

- 损失函数度量 **一次预测** 的好坏，风险函数度量 **平均意义下** 的预测好坏


#### **2.1 损失函数 和 风险函数**

- **损失函数**

1. 度量预测值$f(X)$ 和 真实值 $Y$ 的不一致，记作 $L(Y,f(X))$

2. 分类

    - 0-1 损失函数：$L(Y,f(X)) = \{\frac{1, Y\neq f(X)}{0, Y = f(X)}$

    - 平方损失函数： $L(Y,f(X)) = (Y - f(X))^2$

    - 绝对损失函数： $L(Y,f(X)) = |Y - f(X)|$

    - 对数损失函数： $L(Y,P(Y|X)) = - logP(Y|X)$

<p></p>

- **风险函数**
又称期望损失, expected loss, $R_{exp}(f) $

1. 模型 $f(X)$ 关于联合分布 $P(X,Y)$ 平均意义下的损失，对损失函数求期望

2. $R_{exp}(f) = E_{P}[L(Y,f(X))] = \int_{\mathcal X \times \mathcal Y} L(y,f(x))P(x,y)\mathbf dx\mathbf dy$

3. **病态问题**：期望风险最小学习模型需要用到联合概率分布，但联合概率分布 $P(X,Y)$又是未知的。

<p></p>

- **经验风险**
又称 经验损失，empirical loss, $R_{emp}(f)$

1. $R_{emp}(f) = \frac{1}{N} \sum_{i=1}^NL(y_{i},f(x_i))$

2. 期望风险 $R_{exp}(f)$：关于 **联合分布的期望损失**，对损失函数求期望

3. 经验风险$R_{emp}(f)$：关于 **训练样本集的平均损失**， 对损失函数求平均

4. 样本容量 $N$ 趋于无穷时，后者趋于前者；

5. 现实是，样本数目有限，用后者估计前者时需要矫正： 经验风险最小化 和 结构风险最小化




#### **2.2 经验风险最小化 和 结构风险最小化**

- **经验风险最小化**
又 ERM， empirical risk minimization

1. 认为 经验风险最小的模型就是最优模型

2. $min_{f\in\mathcal F} \frac{1}{N} \sum_{i=1}^NL(y_{i},f(x_i))$

3. 模型=条件概率分布，损失函数=对数损失函数，经验风险最小化=**极大似然估计**

4. 效果未必好，会出现‘过拟合’现象


<p></p>


- **结构风险最小化**
又SRM，structural risk minimization

1. 为防止‘过拟合’，在经验风险上加上 表示模型复杂度的正则化项或罚项， 则为 结构风险

2. 结构风险 $R_{srm}(f) = R_{emp}(f) + \lambda J(f) = \frac{1}{N} \sum_{i=1}^NL(y_{i},f(x_i))  + \lambda J(f) $

    - $J(f)$：模型的复杂度；模型越复杂，复杂度越大；

            - 表示对 复杂模型的惩罚

    - $\lambda$：权衡经验风险和模型复杂度

            - 结构风险需要经验风险和复杂度同时小

3. 模型=条件概率，损失函数=对数损失函数，模型复杂度=模型先验概率，结构风险最小化= **最大后验概率估计**

4. 认为 结构风险最小的模型就是最优模型
5. $min_{f\in\mathcal F} \frac{1}{N} \sum_{i=1}^NL(y_{i},f(x_i))  + \lambda J(f)$



## 1.4 模型评估于模型选择

统计学习的目的是对已知数据和位置数据都能有很好的预测能力

所以，基于损失函数的训练误差 和 测试误差就成为学习方法的评估标准

### **1. 训练误差 和 测试误差**

- **训练误差**

1. 是模型 $Y = \hat f(X)$ 关于 *训练数据集* 的平均损失

2. $R_{emp}(\hat f) = \frac{1}{N} \sum_{i=1}^NL(y_i,\hat f(x_i))$

3. $N$:是训练样本容量


<p></p>

- **测试误差**

1. 是模型 $Y = \hat f(X)$ 关于 *测试数据集* 的平均损失

2. $e_{test} = \frac{1}{N^{'}} \sum_{i=1}^N^{'}L(y_i,\hat f(x_i))$

3. $N^{'}$:是测试样本容量

- 特点：

1. 训练误差：判定一个问题是不是一个统一学习的问题，但本质不重要

2. 测试误差：反应学习方法对未知的测试数据集的预测能力，很重要


### **2. 过拟合和模型选择**

1. 真模型：假设空间中最最最真的那个模型

2. 所选择的模型应与真模型的参数个数相同、参数向量相近

3. 过拟合：选择的模型包含的参数过多，对已知数据预测得很好，但对未知数据预测很差

4. 在选择模型时，应考虑对已知数据的预测能力，还有对未知数据的预测能力

5. 随模型复杂度增加时，训练误差会一直递减，而测试误差呈开口向上的抛物线；

6. 在选择模型时，应使测试误差最小

# 一、 统计学习方法概论2
## 1.5 正则化与交叉验证

用于选择复杂度合适、测试误差最小的模型

- **正则化**

1. 是结构风险最小化的实现

2. 是在经验风险上加一个正则化项 或 罚项

3. 正则化项：一般是模型复杂度的单调递增函数；模型越复杂，正则化值越大；

4. $min_{f\in\mathcal F} \frac{1}{N} \sum_{i=1}^NL(y_{i},f(x_i))  + \lambda J(f) $
        - $\lambda$:调整经验风险和正则化项的系数

5. 正则化项：可以取不同形式；不同的参数向量的范数

6. 正则化作用：选择经验风险与模型复杂度同时较小的模型

7. 奥卡姆剃刀原理：在所有可能选择的模型中，能够很好解释已知数据并十分简单的才是最好的模型；

8. 贝叶斯估计的角度：正则化项对应于模型的先验概率；
        - 假设：复杂的模型有较小的先验概率，简单的模型有较大的先验概率

<p></p>
- **交叉验证**

1. 如果样本数据充足，将数据集分为 

        - 训练集：训练模型
        - 验证集：选择模型
        - 测试集：评估学习方法

2. 选择对验证集有最小预测误差的模型

3. 交叉验证：重复使用数据；将数据切分为训练集与测试集，进行反复训练、测试、模型选择

4. 分类
        - **简单交叉验证**

            - 数据分为：训练集、测试集

            - 用训练集在各种条件下训练模型，得到不同模型；

            - 在测试集评价各个模型的测试误差，选出测试误差最小的模型

        - **S折交叉验证**

            - 将数据随机切分为S个不相交的大小相同的子集；

            - 用S-1个子集数据训练模型，余下子集测试模型；

            - 将这一过程对可能的S种选择重复进行；

            - 宣传平均测试误差最小的模型

        - **留一交叉验证**

            - S折交叉验证的特殊情形 ： S=N

            - N ： 给定数据集的容量









## 1.6 泛化能力

- **泛化误差**

1. 泛化能力：模型对未知数据的预测能力

2. 通过测试误差来评价学习方法的泛化能力，但测试集有限，评价结果可能不可靠

3. 泛化误差：学到的模型 $\hat f$对未知数据预测的误差

4. $R_{exp}(\hat f) = E_P[L(Y,\hat f(X))] = \int_{\mathcal X \times \mathcal Y} L(y,\hat f(x))P(x,y)\mathbf dx \mathbf dy$

6. 就是所学习到的模型的期望风险

<p></p>

- **泛化误差上界**

1. 泛化能力分析通过研究泛化误差的概率上界进行

2. 通过比较两种学习方法的泛化误差上界的大小来比较它们的优劣

3. 性质：

        - 是样本容量的函数，样本容量增加时，泛化上界趋于0；

        - 是假设空间的函数，假设空间容量越大，模型越难学，泛化误差上界越大；

4. 例 二类分类问题：

        - 设 $f$ 是从 $\mathcal F$ 中选取的函数，损失函数是0-1损失

        - 期望风险：$R(f) = E[L(Y,f(X))]$

        - 经验风险：$\hat R(f) = \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i))$

        - 经验风险最小化函数： $f_N = arg min_{f \in \mathcal F} \hat R(f)$

        - 泛化能力：$R(f_N) = E[L(Y, f_N(X))]$

5. 定理：

        - 对二类分类问题，至少以概率 $1-\delta$，

        - 不等式成立：$R(f) \le \hat R(f) + \epsilon(d,N,\delta)$

            - $R(f)$：泛化误差

            - $\hat R(f)$：训练误差

            - $\epsilon(d,N,\delta)$：

                - $\sqrt{\frac{1}{2N}(log d + log \frac{1}{\delta})}$

                - 单调递减：$N$ 趋于无穷，趋于0

                - 是 $\sqrt{log d}$ 阶函数：假设空间$\mathcal F$包含的函数越多，$d$越大，其值越大

        - 训练误差越小的模型，其泛化误差越小





## 1.7 生成模型与判别模型

| bala | 生成 | 判别 |
| ---| ---|---|
|监督学习方法 | 生成方法，generative approach | 判别方法，discriminative approach |
| 所学习到的模型 | 生成模型，generative model | 判别模型， discriminative model | 
| 具体 | 由数据学习联合概率分布$P(X,Y)$,求出条件概率$P(Y|X)$|由数据学习决策函数$f(X)$或条件概率分布$P(Y|X)$作为预测的模型|
||生成模型$P(Y|X) = \frac{P(X,Y)}{P(X)}$
||表示给定输入$X$产生输出$Y$的生成关系 | 给定的输入$X$应该预测什么样的输出$Y$
|例子|朴素贝叶斯法；隐马尔科夫模型 | k近邻法；感知机；决策树；逻辑斯蒂回归模型；最大熵模型；支持向量机；提升方法；条件随机场；
|特点|可还原联合概率分布$P(X,Y)$ | 不能
||学习收敛速度更快，即当样本容量增加时，学习到的模型更快地收敛于真模型 |
|| 存在隐变量时，仍可以使用生成方法学习 | 不行
|||直接学习决策函数$f(X)$或条件概率分布$P(Y|X)$，学习准确率更高，可对数据进行抽象、定义特征、使用特征，简化学习问题



## 1.8 分类问题

1. 输入变量：可离散、可连续；
输出变量：有限个离散值

2. **分类器**：从数据中学习到的一个分类模型或分类决策函数

3. **分类**：利用分类器对新的输入进行输出的预测

4. **类**：可能的输出

5. 分类的类别为多个：多类分类问题，主要讨论二类分类问题

6. 过程：

- 学习过程：学习得到一个分类器 $P(Y|X)$ 或 $Y=f(X)$

- 分类过程：利用分类器对新的输入实例进行分类

7. 类：正类 或 负类

- TP：将正类预测为正类

- FN：将正类预测为负类

- FP：将负类预测为正类

- TN：将负类预测为负类

8. 评价指标

- 分类准确率：正确分类的样本数与总样本数之比

- 精确率：$P = \frac{TP}{TP + FP} = \frac{其中真正为正类}{所有被预测为正类的}$

- 召回率：$R = \frac{TP}{TP+FN} = \frac{其中被预测出为正类的}{所有实际为正类的}$

- $F_1$值：$\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}$ 

        - 是精确率和召回率的调和均值

        - 精确率和召回率都高时， $F_1$值也会高

9. 文本分类：

- 关于文本内容的分类

- 关于文本特点的分类：正面意见、反面意见

- 根据应用的分类：垃圾邮件分类

- 输入文本特征 {可二值0-1、可多值(词频)}，输出文本类别


## 1.9 标注问题

1. 是更复杂的结构预测的简单形式

2. 输入观测序列，输出标记序列或状态序列

3. 可能的标记个数是有限的，但是其组合所成的标记序列的个数呈指数级增长

4. 过程

- 学习过程：

        - 给定数据集：$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$

        - 由输入观测序列，输出标记序列

        - 学习得到一个模型，表示为条件概率分布

        - $P(Y^{(1)},Y^{(2)},...Y^{(n)}| X^{(1)},X^{(2)},...X^{(n)})$

            - $X^{(i)}$：所有可能的观测

            - $Y^{(i)}$：所有可能的标记

- 标注过程

        - 按照学习得到的条件概率模型，对新的输入观测序列找到相应的输出标记序列

        - 等价于，对一个观测序列 $x_{N+1} = (x_{N+1}^{(1)},x_{N+1}^{(2)},...,x_{N+1}^{(n)})^T$，找到使条件概率$P(y_{N+1}^{(1)},y_{N+1}^{(2)},...,y_{N+1}^{(n)})^T|(x_{N+1}^{(1)},x_{N+1}^{(2)},...,x_{N+1}^{(n)})^T$ 最大的标记序列 $y_{N+1} = (y_{N+1}^{(1)},y_{N+1}^{(2)},...,y_{N+1}^{(n)})^T$

5. 词型标注

- 对句子中每个单词进行词型标注；即对一个单词序列预测其对应的词型标记序列

- 单词 = 观测， 句子 = 观测序列

- 标记：名词短语的开始B、结束E或其他O

- 标记序列：句子中基本名称短语的所在位置


## 1.10 回归问题

1. 预测输入变量和输出变量之间的关系

2. 等价于函数拟合：选一条能很好拟合已知数据并预测未知数据的曲线

3. 过程：
- 学习过程：
        - 学习得到一个模型，即函数$Y=f(X)$

- 预测过程
        - 对新输入 $x_{N+1}$，根据模型$Y=f(X)$确定相应的输出$y_{N+1}$

4. 分类

- 按输入变量个数：一元回归、多元回归

- 按输入输出变量关系：线性回归、非线性回归

5. 常用损失函数：平方损失函数，可用最小二乘法求解

6. 可以股价预测balabala的
